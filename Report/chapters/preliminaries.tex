%---------------------------------------------------------------------------------------------
%‌ Chapter 2
%---------------------------------------------------------------------------------------------
\فصل{طبقه‌بند ها}
در این فصل ابتدا به معرفی چند طبقه‌بند مهم می‌پردازیم و ویژگی‌ها، کاربردها و نحوه عملکرد و پیاده‌سازی آنها را برسی می‌کنیم.


\قسمت{طبقه‌بند K-NN}
در شناسایی الگو\پاورقی {Pattern Recognition} $K$-نزدیک‌ترین همسایه\پاورقی {k-Nearest Neighbor} یک متد آمار ناپارامتری است که برای طبقه‌بندی آماری\پاورقی {Statistical classification} و رگرسیون\پاورقی {Regression} استفاده می‌شود. در هر دو حالت $K$ شامل نزدیک‌ترین مثال آموزشی در فضای داده‌ای می‌باشد و خروجی آن بسته به نوع مورد استفاده در طبقه‌بندی و رگرسیون متغیر است. در حالت طبقه‌بندی با توجه به مقدار مشخص شده برای $K$، به محاسبه فاصله نقطه ای که می‌خواهیم برچسب آن را مشخص کنیم با نزدیک‌ترین نقاط می‌پردازد و با توجه به تعداد رای حداکثری این نقاط همسایه، در رابطه با برچسب نقطه مورد نظر تصمیم‌گیری می‌کنیم.

برای محاسبه این فاصله می‌توان از روش‌های مختلفی استفاده کرد که یکی از مطرح‌ترین این روش‌ها، فاصله اقلیدسی «شکل \رجوع {شکل:فاصله اقلیدسی}»است. در حالت رگرسیون نیز میانگین مقادیر به‌دست آمده از کی خروجی آن می‌باشد. از آنجا که محاسبات این الگوریتم بر اساس فاصله است نرمال‌سازی داده‌ها می‌تواند به بهبود عملکرد آن کمک کند. ~\مرجع{roleofdata} و ~\مرجع{elementsofstatisticallearning} 

\شروع{شکل}[h]
\centerimg{Euclidean_distance_2d}{9cm}
\شرح{استفاده از قضیه فیثاغورس برای محاسبه فاصله اقلیدسی دو بعدی}
\برچسب{شکل:فاصله اقلیدسی}
\پایان{شکل}

\زیرقسمت{الگوریتم}
داده‌های اولیه، بردارهایی در یک فضای چند بعدی هستند که هر کدام شامل برچسبی\پاورقی{Label} به نام دسته می‌باشند.

فاز یادگیری\پاورقی {Training Phase} الگوریتم، شامل ذخیره‌سازی بردارهای ویژگی و برچسب دسته نمونه‌های اولیه است.

در فاز طبقه‌بندی، $K$ یک ثابت توسط کاربر تعریف می‌شود و بردار بدون برچسب «نقطه تست» از دسته ای است که بیشترین تعداد را در $K$ نزدیک‌ترین همسایه آن نقطه داشته باشد. به این ترتیب برچسب نقطه تست نیز مشخص می‌شود.
معیار فاصله برای متغیرهای پیوسته معمولاً فاصله اقلیدسی است.

اگر K-NN را با استفاده از الگوریتم‌های تخصصی مانند تجزیه و تحلیل اجزای همسایه\پاورقی {Neighbourhood Components Analysis} یا حاشیه بزرگ نزدیک‌ترین همسایه\پاورقی {Large Margin Nearest Neighbor} پیاده‌سازی کرد، می‌توان دقت اندازه‌گیری را به شدت بهبود داد.

مراحل الگوریتم K-NN شامل موارد زیر خواهد بود:

\شروع{الگوریتم}{K-NN}

\ورودی ماتریس
\خروجی نزدیک‌ترین همسایه

\دستور داده‌های را بارگیری کنید
\دستور $K$ به عنوان تعداد نزدیک‌ترین همسایگان انتخاب کنید.
\دستور برای هر یک از داده‌های اولیه:
\دستور فاصله بین داده مورد سؤال و هر یک داده‌های اولیه را محاسبه کنید
\دستور فاصله و اندیس نمونه را به یک مجموعه اضافه کنید
\دستور مجموعه را بر اساس فاصله از کوچک به بزرگ مرتب کنید
\دستور نقاط $K$ عضو اول مجموعه مرتب شده را انتخاب کنید
\دستور بسته به حالت یا حالت طبقه‌بندی، خروجی را اعلام کنید

\پایان{الگوریتم}

\زیرقسمت {انتخاب پارامتر}
بهترین انتخاب $K$ بستگی به داده‌ها دارد. به‌طور کلی، مقادیر بزرگ $K$ باعث کاهش خطا در طبقه‌بندی می‌شود، اما وضوح مرز بین کلاس‌ها را کمتر می‌کند. $K$ مناسب را می‌توان با استفاده از تکنیک‌های مختلف انتخاب کرد. مورد خاص زمانی است که دسته پیش‌بینی شده برای عضو جدید، همان دسته نزدیکترین نمونه باشد. « به ازای $ K=1 $ ». در این صورت، الگوریتم نزدیکترین همسایه نامیده می‌شود.

دقت الگوریتم K-NN می‌تواند در نتیجه وجود خطا، یا ویژگی‌های غیر مرتبط یا اگر مقیاس داده‌ها با اهمیت آن‌ها تطابق نداشته باشد، به شدت تضعیف می‌شود. برای بهبود طبقه‌بندی، تلاش‌های زیادی در زمینه انتخاب یا مقیاس بندی داده‌ها شده‌است. یک رویکرد بسیار مشهور، استفاده از الگوریتم‌های تکاملی برای بهینه‌سازی مقیاس بندی داده‌ها است.

در دسته‌بندی‌های دو کلاس، بهتر است که $K$ یک عدد فرد انتخاب شود، زیرا این امر از گره خوردن آرا جلوگیری می‌کند.

\زیرقسمت{مزایا و معایب}

\مهم{مزایا:}

\شروع{فقرات}

\فقره هیچ پیش فرضی در مورد داده‌ها وجود ندارد - مثال برای داده‌های غیر خطی
\فقره الگوریتم ساده
\فقره دقت نسبتاً بالا
\فقره مقایسه مدل‌های یادگیری تحت نظارت بهتر
\فقره چند منظوره - برای طبقه‌بندی و رگرسیون

\مهم{مضرات:}

\فقره محاسبه گران
\فقره نیاز به حافظه بالا - چرا که الگوریتم، تمام داده‌های قبلی را ذخیره می‌کند
\فقره ذخیره تقریباً یا همه داده‌های اولیه
\فقره مرحله پیش‌بینی ممکن است کند باشد « با N بزرگ »
\فقره حساس به ویژگی‌های نامناسب و مقیاس داده

\پایان{فقرات}


\قسمت{طبقه‌بند آبشاری و ویژگی‌های LBP}

\زیرقسمت{طبقه‌بند آبشاری}
برای اولین بار، ایده تشخیص اشیاء مبتنی بر طبقه‌بند آبشاری\پاورقی {Cascade Classifier} و با استفاده از ویژگی‌های هار\پاورقی{Haar} توسط پاول ویولا\پاورقی {Paul Viola} و مایکل جونز\پاورقی {Michael Jones} در مقاله ای تحت عنوان: 
\کد{Rapid Object Detection using a Boosted Cascade of Simple Features}\زیرنویس{ نسخه اصلی مقاله از لینک زیر قابل دریافت است: \href{https://b2n.ir/reference_Haar}{\کد{b2n.ir/reference\_Haar}}} درسال 2001 مطرح شد. رویکرد این مقاله بر اساس یادگیری ماشین\پاورقی {Machine Learning} است که در آن یک تابع آبشاری با تعداد زیادی تصویر مثبت\زیرنویس{ تصویر چهره} و منفی\زیرنویس{ تصویر بدون چهره} آموزش داده می‌شود. ~\مرجع{rapid}

\شروع{شکل}[h]
\centerimg{Haar_Feature}{8cm}
\شرح{فیلترهای مورد استفاده برای استخراج ویژگی}
\برچسب{شکل:ویژگی‌های هار}
\پایان{شکل}

\شروع{شکل}[h]
\centerimg{Haar_good_feature}{8cm}
\شرح{استخراج ویژگی های خوب}
\برچسب{شکل:ویژگی‌های خوب هار}
\پایان{شکل}

به‌دلیل آن‌که مسئله ما تشخیص چهره است،‌ در ابتدا برای آموزش شبکه به تعداد زیادی تصاویر مثبت و منفی نیاز داریم. در گام دوم باید از تصاویر ویژگی استخراج کنیم.
برای استخراج ویژگی،‌ از ویژگی های «هار» که در «شکل \رجوع{شکل:ویژگی‌های هار}» نشان داده شده است استفاده می‌شود. این ویژگی‌ها دقیقا مانند هسته های فیلترهای کانولوشنی هستند.


با استخراج ویژگی‌ها، ‌تعداد زیادی از ویژگی ها مطلوب ما نیستند و اطلاعات مفیدی به ما نمی‌دهند. برای مثال، «شکل \رجوع{شکل:ویژگی‌های خوب هار}» دو نمونه ویژگی خوب را نشان می‌دهد.
معمولا ناحیه فرورفتگی چشم‌ها تیره تر از ناحیه بالای بینی و گونه‌ها است. ویژگی اول به همین موضوع اشاره دارد. ویژگی دوم، ‌بر این خاصیت متکی‌است که چشم‌ها تیره‌تر از ناحیه بینی است.
اما اگر همین فیلتر‌ها در محل های دیگری قرار گیرند، ‌ویژگی خوبی استخراج نمی‌کنند.


برای آموزش،‌ تک تک ویژگی‌ها را روی تمام تصاویر آموزشی اعمال می‌کنیم. برای هر ویژگی، بهترین آستانه را پیدا می‌کنیم به طوری که چهره‌ها به دو دسته مثبت و مفی طبقه بندی کند. بدیهسیت که ممکن است طبقه‌بندی های اشتباهی صورت بگیرد که موجب ایجاد خطا می‌شود. ما باید ویژگی هایی با کمترین میزان خطا را انتخاب کنیم،‌ یعنی ویژگی هایی را انتخاب کنیم که تصاویر چهره و غیر چهره را با دقت بیشتری طبقه‌بندی کنند.

فاز اول فرایند آموزش بدین صورت است که ابتدا به هر تصویر وزن‌های برابر داده می‌شود. پس از هر دوره آموزش وزن تصاویر طبقه‌بندی اشتباه افزایش می‌یابد. فرایند تا زمانی تکرار می‌شود که خطا یا دقت به مقدار مورد نظر ما برسد و بهترین ویژگی ها را پیدا کنیم.


طبقه‌بند نهایی، مجموع تعدادی طبقه بند ضعیف است که در مراحل قبل بدست آمده است. می‌گوییم ضعیف چون به تنهایی نمی‌توانند تصویر را طبقه‌بندی کنند اما اگر با سایر طبقه‌بندها ترکیب شوند، طبقه‌بندی قوی تشکیل می‌دهند به طوری که با ترکیب 200 ویژگی، دقت تشخیص به حدود \%95 می‌رسد.


شرکت OpenCV بهترین ویژگی‌های بدست آمده\زیرنویس{ فایل‌های نام برده از این لینک قابل دریافت است: \href{https://github.com/opencv/opencv/tree/master/data/haarcascades}{\کد{github.com/opencv/opencv/tree/master/data/haarcascades}}} را برای تشخیص چهره، چشم، بدن و ... درقالب فایل‌هایی با پسوند \کد{.xml} ارائه کرده است.


در این پروژه ما از فایل \کد{haarcascade\_frontalcatface.xml} استفاده کردیم. در فصل بعد به برسی مقدمات و الزامات انجام پروژه مانند معرفی سخت‌افزار، نصب سیستم‌عامل و ... می‌پردازیم.

\زیرقسمت{الگوهای باینری محلی}

الگو‌های باینری محلی\پاورقی {Local Binary Pattern} یا به‌طور اخصار LBP اولین بار در سال 2004 توسط آهُنن\پاورقی {Ahonen} در مقاله ای تحت عنوان \کد{Face Recognition with Local Binary Patterns} ارائه شد\زیرنویس{ فایل مقاله از این لینک قابل دریافت است: \href{https://b2n.ir/reference_Local}{\کد{b2n.ir/reference\_Local}}}. 

نحوه عملکرد LBP بدین صورت است که یک پیکسل از تصویر طبق یک آستانه\پاورقی {Threshold} با پیکسل های همسایه خود مقایسه می‌شود و مقدار جدید پیکسل ها ساخته می‌شود.

اولین مرحله در LBP، تبدیل تصویر \کد{RGB} به \کد{Grayscale} است. برای تمامی پیکسل های تصویر خاکستری، یک همسایگی به اندازه شعاع \کد{r} از پیکسل مرکزی انتخاب می‌کنیم. انتخاب می‌کنیم. «شکل \رجوع{شکل:همسایگیLBP}» سپس یک مقدار جدید LBP برای پیکسل مرکزی محاسبه می‌شود و در آرایه 2 بعدی خروجی با همان عرض و ارتفاع تصویر ورودی ذخیره می‌شود. «شکل \رجوع{شکل:عملکردLBP}» ~\مرجع{Multiresolution}

\شروع{شکل}[h]
\centerimg{LBP_Circular}{10cm}
\شرح{همسایگی در LBP}
\برچسب{شکل:همسایگیLBP}
\پایان{شکل}



\شروع{شکل}[h]
\centerimg{LBP_Coding}{15cm}
\شرح{عملکرد LBP}
\برچسب{شکل:عملکردLBP}
\پایان{شکل}

مقدار جدید بدست آمده برای پیکسل وسط جایگزین مقدار قبلی آن می‌شود. این عمل برای تمامی پیکسل های تصویر تکرار شده و مقادیر پیکسل‌ها آپدیت می‌شود. در نهایت تمامی ویژگی‌های بدست آمده در یک بردار\پاورقی {Vector} کنار هم چیده می‌شود و بردار ویژگی‌های LBP را می‌سازند. ~\مرجع{Facerecognitionwithlbp}

در شکل‌های «\رجوع{شکل:عملیLBP} و \رجوع{شکل:بردار ویژگی LBP} » نمونه‌ای عملی از استخراج ویژگی‌های LBP با همسایگی 8 پیکسل و شعاع همسایگی 1.5 آورده شده است: «جهت برسی کد این بخش، به پیوست آ مراجعه کنید»

\شروع{شکل}[h]
\centerimg{LBP_Implementation1}{14cm}
\شرح{پیاده‌سازی عملی LBP}
\برچسب{شکل:عملیLBP}
\پایان{شکل}

\شروع{شکل}[h]
\centerimg{LBP_Implementation2}{15cm}
\شرح{بردار ویژگی‌های LBP}
\برچسب{شکل:بردار ویژگی LBP}
\پایان{شکل}


\زیرقسمت{الگوهای باینری محلی برای الگوریتم شناسایی چهره}

با توجه به دیتاست تصاویر موجود، در اولین مرحله الگوریتم، چهره را به سلول‌های $ 7\times7 $ تقسیم میکنیم. «شکل\رجوع{شکل:تقسیم چهره به سلول}» سپس برای هرکدام از این سلول‌ها هیستوگرام LBP را محاسبه می‌کنیم. با محاسبه هیستوگرام برای هر یک از سلول ها، در واقع می‌توان اطلاعاتی مانند چشم‌ها، بینی و دهان را از تصویر استخراج کرد. «شکل\رجوع{شکل:وزن هیستوگرام}»

\شروع{شکل}[h]
\centerimg{LBP_7_7}{5cm}
\شرح{تقسیم چهره به سلول های $ 7\times7 $}
\برچسب{شکل:تقسیم چهره به سلول}
\پایان{شکل}

\شروع{شکل}[h]
\centerimg{LBP_feature}{10cm}
\شرح{تصویر اصلی چهره «سمت چپ» و وزن‌های استخراج شده «سمت راست»}
\برچسب{شکل:وزن هیستوگرام}
\پایان{شکل}

وزن هیستوگرام LBP برای سلول های سفید رنگ «مثل چشم‌ها» 4 برابر بیشتر از سایر سلول‌ها است. این بدین معنی است که ما هیستوگرام های LBP را از نواحی سلول‌های سفید رنگ می‌گیریم و آنها را در 4 ضرب می کنیم.

وزن سلول‌های خاکستری روشن «مثل دهان و گوش‌ها» 2 در نظر گرفته می‌شود.

وزن سلول‌های خاکستری تیره «مثل داخل گونه و پیشانی» 1 درنظر گرفته می‌شود.

و درنهایت وزن سلول‌های سیاه «مثل بینی و خارج گونه‌ها» کاملا نادیده در نظر گرفته می‌شود و وزن آن‌ها 0 است.

مقادیر وزن‌های نام برده شده در بالا کاملا به‌طور تجربی توسط آهُنن پیدا شد.

در نهایت هیستوگرام وزن های بدست آمده به یکدیگر متصل می‌شوند تا بردار ویژگی نهایی را تشکیل دهند.

عمل تشخیص چهره با استفاده از فاصله $\chi^{2}$ و طبقه‌بند نزدیک‌ترین همسایه\پاورقی {Nearest Neighbor} به صورت زیر انجام می‌شود:

\شروع{فقرات}

\فقره چهره به سیستم داده می‌شود

\فقره LBP ها به روشی که توضیح داده شد استخراج شده و به وزن تبدیل می‌شوند و به هم متصل شده تا بردار ویژگی ها را بسازند.

\فقره KNN با $ K=1 $ و با فاصله $\chi^{2}$ برای یافتن نزدیک‌ترین چهره در دیتاست انجام می‌شود.

\فقره نام شخص مرتبط با چهره با کمترین فاصله $\chi^{2}$ به عنوان طبقه بندی نهایی انتخاب می شود.

\پایان{فقرات}